{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmYassir/ml_dl_algorithms/blob/main/KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmxfpdhG42L"
      },
      "source": [
        "# A little theory on kNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1BBCkJoG42O"
      },
      "source": [
        "## Intuition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFm85oIiG42P"
      },
      "source": [
        "The algorithm $k$ Nearest Neighbours is one of the simpler machine learning algorithms. It is motivated by the idea that similar examples $x_t$ should have similar targets $y_t$.\n",
        "\n",
        "So, to predict the target class of a test examples $x$, all we need to do is find the $k$ nearest neighbours to $x$ using some metric - for example, euclidean distance also known as $L_2$ norm, or more generally minkowski distance $L_p$.\n",
        "\n",
        "In a classification problem, we use those $k$ nearest neighbours to predict the target class of $x$ as the most common target of its neighbours i.e. it's as though each neighbour of $x$ casts a vote for their own target class and the class with the most votes wins.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJAsbLyPG42W"
      },
      "source": [
        "## Mathematical Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ6RVf7tG42Y"
      },
      "source": [
        "Let\n",
        "\n",
        "* $x$ be test examples\n",
        "* $m$ be the number of classes\n",
        "* $D_n = \\{(x_i,y_i)\\}_{i=1}^n$ be the training data where $y_i \\in Y=\\{1,\\dots,m\\}$ is the corresponding target class of example $x_i$\n",
        "* $d(\\dot{},\\dot{})$ be our distance metric\n",
        "* $V(x, k)$ is the set of $k$ nearest neighbours of $x$\n",
        "\n",
        "<br/>\n",
        "\n",
        "The prediction by the $k$-NN algorithm is therefore:\n",
        "\n",
        "> $$f(x)={\\mbox{arg max}} \\left(\\frac{1}{k} \\sum_{(x_i,y_i) \\in V(x, k)} \\mathrm{onehot}_{m}(y_i)\\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fWJjTPHVwee"
      },
      "source": [
        "To find out the $k$ nearest neighbours, we need to calculate some \"distance\" between the test sample $x$ and all the samples in the training set, and pick the $k$ samples with the least distance from $x$.\n",
        " \n",
        "A common distance function is the euclidean distance function:\n",
        "> $$d(a,b)= \\sqrt{\\sum_{i=1}^{dim}(a_i-b_i)^2}$$\n",
        "\n",
        "which is a specific case of the $L_p$ norm of Minkowski (where $p = 2$)\n",
        "> $$d(a,b)= \\left(\\sum_{i=1}^d|a_i-b_i|^p\\right)^\\frac{1}{p}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp9Q6E5cG42c"
      },
      "source": [
        "## Pseudocode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLhAaXiRG42d"
      },
      "source": [
        "We define a machine learning algorithm by specifying its training procedure for some training data and how to predict the target for a test example. Given that the training procedure for $k$-NN is simply loading the training data $D_n$, we can specify how to predict the target class for the case when $k = 1$:\n",
        "\n",
        "    def kNN_1(x):\n",
        "        min = +inf # intialize the distance of the nearest neighbour\n",
        "        idx = -1 # initialize the index of the nearest neighbour\n",
        "        \n",
        "        for i=1 to n:\n",
        "            dt = d(X[i], x)\n",
        "            if dt < min\n",
        "                min = dt\n",
        "                idx = i\n",
        "                \n",
        "        return Y[idx]\n",
        "\n",
        "This runs in $O(n(k+d))$ time but you can get $O(n(log(k)+d))$ time by using a priority queue (heap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9J6ddy0G42e"
      },
      "source": [
        "# Let's practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9JHG9QLG42f"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKBwCm8VG42h"
      },
      "source": [
        "We want to make a machine learning algorithm to identify flowers. We have three types of iris species and we will try to use the characteristics of each flower (features) to determine which species of iris it is (class). But you don't know anything about flowers! So we will learn this algorithm using a dataset of flower measurements and the classes those flowers correspond to (training data), and we will use 1-kNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UtjXydlG42h"
      },
      "source": [
        "## 1. Calculate the $L^p$ (Minkowski) distance between two vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSlrdn_6G42j"
      },
      "source": [
        "We want a function that given two vectors (`np.array`) will output the Minkowski distance between them. Complete the function `minkowski_vec` below. Test it yourself on two vectors (you can import the iris dataset as we did in the tutorial to use real iris vectors).\n",
        "\n",
        "### ***Ex: Calculate Minkowski distance between two vectors***. Here is the [definition](http://en.wikipedia.org/wiki/Minkowski_distance) in case you need it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBtUWejJG42k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922caf93-9e1a-41c4-f163-53cf8db66bae"
      },
      "source": [
        "import numpy as np\n",
        "def minkowski_vec(x1, x2, p=2.0):\n",
        "    # insert code here /insérer votre code ici\n",
        "    return (np.abs(x1 - x2)**p).sum()**(1.0/p)\n",
        "\n",
        "# for testing\n",
        "a = np.ones((4,))\n",
        "b = np.zeros((4,))\n",
        "print(minkowski_vec(a, b))\n",
        "print(minkowski_vec(a, a))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gLdLU9G42q"
      },
      "source": [
        "**Note:** since this is a vector, we'll need to apply operations to the different dimensions. We could do this by iterating over the array one element at a time (e.g. here to calculate the difference in absolute values)\n",
        "\n",
        "    s = 0\n",
        "    for i in range(x1.shape[0]):\n",
        "        s = s + abs(x1[i] - x2[i])\n",
        "\n",
        "or we could use `numpy` intelligently to do the same thing:\n",
        "\n",
        "    s = np.sum(np.abs(x1 - x2))\n",
        "\n",
        "the difference is that the second option is not just more compact and easy to read, it uses `numpy`'s library to calculate the sums and operations which is much much faster than Python because they are specialized math functions written in C++. \n",
        "\n",
        "In short, use numpy functions instead of for loops where possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF27CGFvG42r"
      },
      "source": [
        "## 2. Calculate $L^p$ distance between a vector and a matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1soCeiIEG42r"
      },
      "source": [
        "We also need a function to compare one flower to a bunch of other flowers. We will modify the function `minkowski_mat` to calculate an $L_p$ distance between a single vector (or 1D `np.array`) and a matrix (or 2D `np.array`). This function should return an array of distances corresponding to the distance between the single flower $x1$ and each other flower in the bunch represented as rows in $X2$.  Use `np.newaxis` or broadcasting rules.\n",
        "\n",
        "### ***Ex: Calculate Minkowski distance between a vector and a matrix***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMkQ25MG42t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d0c94d-9cab-4603-dc4d-3ab5d3a926ba"
      },
      "source": [
        "def minkowski_mat(x1, X2, p=2.0):\n",
        "    # insert code here/ insérer votre code ici\n",
        "    dist = (np.abs(x1 - X2)**p).sum(axis=1)**(1.0/p)\n",
        "    return dist\n",
        "\n",
        "# for testing\n",
        "a = np.ones((4,))\n",
        "b = np.random.randint(5, size=(10,4))\n",
        "print(minkowski_mat(a,b))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.87298335 4.47213595 3.60555128 3.74165739 3.31662479 3.16227766\n",
            " 3.74165739 3.16227766 3.87298335 1.73205081]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wYgZSK5G426"
      },
      "source": [
        "Reiterating the very important point **the vast majority of vector-vector, vector-matrix, or matrix-matrix operations are going to be much more efficient in numpy than in python using a for loop**. \n",
        "\n",
        "You may have noticed that the difference in the efficient example implementations of `minkowski_vec` and `minkowski_mat` is just the part: `axis=1`. This exercise is to understand why it is important (and necessary) to specify on which axis you are applying the sum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBuDQBOhG427"
      },
      "source": [
        "## 3. 1-KNN / 1-PPV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kux7hYpcG428"
      },
      "source": [
        "We're nearly there!\n",
        "\n",
        "### ***Ex: Finish the following function to predict the species of a test data point with features `x`:***\n",
        "- From `data`, slice the `features` and `target`\n",
        "- Calculate the distance between `x` and the `features` using `minkowski_mat`\n",
        "- Find the data point with the least distance, and return its `target`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3TvSv5cG42-"
      },
      "source": [
        "def knn(x, feats, targets, p=2): \n",
        "    # insert code here/ insérer votre code ici\n",
        "    dist = minkowski_mat(x, feats, p)\n",
        "    return targets[np.argmin(dist)]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCltbAmHG43E"
      },
      "source": [
        "Note that `x` is the array of features (no target) of the test example. This functions should be quite easy to write then because `minkowski_mat` will output an array of distances, and we want to find the *index* of the *minimum* of those distances, then just find the target at that index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-plNqvtQG43G"
      },
      "source": [
        "## Conclusion / Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhuQMZ1rG43I"
      },
      "source": [
        "We now have all the components of the 1-NN algorithm, all that's left is to put it all together and test it\n",
        "\n",
        "Remember that we can access the functions we've executed in the previous cells\n",
        "\n",
        "After testing your implementation, write a `for` loop which will call `knn(iris[i,:-1], iris, p)` for each example `i` and compare the prediction to the true target `iris[i,-1]`. The two should always be the same, why?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-NH1XPDG43K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd73ec5e-f48b-4b1c-ef84-c9f4512c0af8"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "train_data, train_targets = iris['data'], iris['target']\n",
        "\n",
        "#iris = np.loadtxt('iris.txt')\n",
        "\n",
        "predictions = np.zeros(train_data.shape[0])\n",
        "#for i in range(iris.shape[0]):\n",
        "for i,elem in enumerate(train_data):\n",
        "    #predictions[i] = knn(iris[i,:-1],iris)\n",
        "    predictions[i] = knn(elem,train_data, train_targets)\n",
        "    \n",
        "#targets = iris[:,-1]\n",
        "print(\"error rate:\",(1.0-(predictions==train_targets).mean())*100.0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P74PpuHdG43U"
      },
      "source": [
        "## Bonus and things to reflect on for the next time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbwhn39uG43W"
      },
      "source": [
        "In machine learning, we usually have a split of training data and testing data\n",
        "* divide the dataset in two : one training dataset with 50 examples (randomly sampled) and a testing dataset with the remaining examples. \n",
        "* Using the training data to find the nearest neighbours and target output for a given example, calculate the performance of your algorithm on training and testing. Why is there such a difference? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4VXtbh7G43X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738aeabc-798c-414f-ddf0-f7166a3cf8c9"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "feats, targets = iris['data'], iris['target']\n",
        "\n",
        "indexes = np.arange(feats.shape[0])\n",
        "# set the random seed so we have exact reproducibility\n",
        "np.random.seed(3395)\n",
        "np.random.shuffle(indexes)\n",
        "\n",
        "train_feats, train_targets = feats[indexes[:50]], targets[indexes[:50]]\n",
        "test_feats, test_targets = feats[indexes[50:]], targets[indexes[50:]]\n",
        "\n",
        "# predictions on the training set\n",
        "train_predictions = np.zeros(train_feats.shape[0])\n",
        "for i, elem in enumerate(train_feats):\n",
        "    train_predictions[i] = knn(elem, train_feats, train_targets)\n",
        "    \n",
        "# predictions on the testing set\n",
        "test_predictions = np.zeros(test_feats.shape[0])\n",
        "for i, elem in enumerate(test_feats):\n",
        "    test_predictions[i] = knn(elem, test_feats, test_targets)\n",
        "    \n",
        "print(\"Training data error rate\", (1.0-(train_predictions==train_targets).mean())*100.0)\n",
        "print(\"Testing data error rate\", (1.0-(test_predictions==test_targets).mean())*100.0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data error rate 0.0\n",
            "Testing data error rate 0.0\n"
          ]
        }
      ]
    }
  ]
}